# Python Markdown Library Comparison

## Overview
This report compares **cmarkgfm** (GitHub’s C-based CommonMark/GFM engine) against four popular pure-Python Markdown libraries: **Python-Markdown**, **markdown2**, **mistune**, and **commonmark.py**. It includes a feature survey and a local performance benchmark with charts.

## Libraries surveyed
- **cmarkgfm**: Python bindings to GitHub’s `cmark-gfm` for CommonMark + GFM rendering, with options such as safe rendering and footnotes.
- **Python-Markdown**: Reference-style Markdown implementation with an extensive extensions ecosystem.
- **markdown2**: Markdown.pl-compatible implementation with a large set of extras (tables, fenced code, footnotes, etc.).
- **mistune**: Fast parser with a plugin system (tables, task lists, footnotes, strikethrough, etc.).
- **commonmark.py**: CommonMark-compliant parser (deprecated, but still widely referenced).

## Feature comparison (from source/README review)
| Library | Implementation | Spec focus | GFM support | Extensibility | Notes |
| --- | --- | --- | --- | --- | --- |
| cmarkgfm | C library bindings | CommonMark + GFM | Yes (via GitHub-flavored render) | Limited (C options) | Safe/unsafe rendering options, footnotes, GFM tables. |
| Python-Markdown | Pure Python | Markdown (Gruber) | Via extensions | Extensive extensions | Extensions include tables, fenced code, footnotes, etc. |
| markdown2 | Pure Python | Markdown.pl behavior | Partial (extras) | Extras system | Extras include tables, fenced code blocks, footnotes, strike, etc. |
| mistune | Pure Python | Markdown | Via plugins | Plugin system | Plugins include tables, task lists, footnotes, strikethrough, etc. |
| commonmark.py | Pure Python | CommonMark | No GFM | Limited | CommonMark spec parser; project deprecated. |

## Benchmark methodology
- **Machine**: container runtime (shared CPU)
- **Iterations**: 20 runs per library & size (3 warmup runs)
- **Input sizes** (repeated mixed-feature sample):
  - **Small**: 2 sections (~1.2 KB)
  - **Medium**: 20 sections (~12.8 KB)
  - **Large**: 120 sections (~76.7 KB)
- **Renderer config**:
  - `cmarkgfm`: `github_flavored_markdown_to_html`
  - `Python-Markdown`: `fenced_code`, `tables`, `footnotes`, `sane_lists`, `smarty`, `toc`
  - `markdown2`: `fenced-code-blocks`, `tables`, `footnotes`, `strike`, `smarty-pants`, `toc`
  - `mistune`: plugins `strikethrough`, `table`, `task_lists`, `footnotes`
  - `commonmark`: default CommonMark renderer

Benchmark script and raw results are included in this folder.

## Performance charts
Charts are generated by `benchmark.py` into the `charts/` directory when you run the benchmark locally. (The binary PNG outputs are not stored in this repository.)

## Large-input summary
| Library | Mean ms (large) | P95 ms (large) | Throughput MB/s |
| --- | ---: | ---: | ---: |
| cmarkgfm | 11.81 | 13.13 | 6.50 |
| mistune | 110.19 | 127.61 | 0.70 |
| commonmark | 235.01 | 270.93 | 0.33 |
| markdown2 | 312.62 | 331.18 | 0.25 |
| markdown | 508.09 | 533.87 | 0.15 |

## Files
- `benchmark.py`: benchmark runner and chart generator.
- `requirements.txt`: dependencies used for the benchmark.
- `data/benchmark_results.csv`: raw results.
- `charts/`: generated PNG charts (created locally when running the benchmark).

## Reproduce
```bash
python -m pip install -r requirements.txt
python benchmark.py
```
